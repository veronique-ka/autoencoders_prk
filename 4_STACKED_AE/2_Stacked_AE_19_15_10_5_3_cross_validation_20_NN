import timeit
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split, StratifiedKFold
from keras.layers import Input, Dense
from keras.models import Model
from keras.models import Sequential
from keras.optimizers import SGD
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error
from statistics import median

# Set start time
start_time = timeit.default_timer()

# Load data
prk = pd.read_csv("https://raw.githubusercontent.com/veronique-ka/autoencoders_prk/master/prk_reord_bin.csv")

#Convert pandas dataframe to numpay array
p=np.array(prk)

x=p[:,4:20]
y=p[:,20]
        
# Normalize values between 0 and 1
min_max=MinMaxScaler()
x=min_max.fit_transform(x)

# Initiate lists of metrics for test-set, based on 5 iterations
MSE_test_5 = list()
R2_test_5 = list()
MAE_test_5 = list()

# Initiate 5 itereations for K-folds partitions
for i in range(0,5):
    print("Iteration number",i)
    
    # Define k-fold validation splits
    def load_data_kfold(k):                  

        folds = list(StratifiedKFold(n_splits=k, shuffle=True).split(x, y))
        
        return folds, x, y
    
    k = 10
    folds, x, y = load_data_kfold(k)         
    
    
    # initiate lists of metrics based on individual fold's metrics
    MSE_valid_folds = list()
    R2_valid_folds = list()
    MAE_valid_folds = list()
        
    # initiate partitions into train and validation subsets
    for j, (train_idx, val_idx) in enumerate(folds):
        print('\nFold ',j)
        x_train_cv = x[train_idx]
        y_train_cv = y[train_idx]
        x_valid_cv = x[val_idx]
        y_valid_cv= y[val_idx]  
               
        # Start Layer-wise pretraining
    
        # Initiate a list of encoders
        encoders = []
    
        # Define architecture
        nb_hidden_layers = [19,15,10,5,3]
        # Copy data to temporary datasets 
        x_train_tmp = np.copy(x_train_cv)
        x_test_tmp = np.copy(x_valid_cv)
    
        # Start layer-wise training by interacting each pair of layers within a deep structure 
        for k, (n_in, n_out) in enumerate(zip(nb_hidden_layers[:-1], nb_hidden_layers[1:]), start=1):
            # Print numbers of layers being trained
            print('Training the layer {}: Input {} -> Output {}'.format(k, n_in, n_out))  
    
            # Set a vanilla AE between each pair of layers        
            inputs = Input(shape=(n_in,))
            encoded = Dense(n_out, activation='sigmoid')(inputs)
            decoded = Dense(n_in, activation='sigmoid')(encoded)
            autoencoder = Model(inputs, decoded)
            autoencoder.summary()

            # Compile AE
            autoencoder.compile(loss='binary_crossentropy', optimizer='adam')
        
            # Train AE to reconstruct
            autoencoder.fit(x_train_tmp, x_train_tmp, batch_size=64, nb_epoch=200, validation_data=(x_test_tmp, x_test_tmp))
        
            # Store trainined weights and update training data
            encoders.append(autoencoder.layers[0])
        
            # Get a separate model of Encoder
            encoder = Model(inputs, encoded)
        
            # Get a code or encoded representation in a pair of layers
            encoded_train=encoder.predict(x_train_tmp)
            encoded_test=encoder.predict(x_test_tmp)
        
            # Assign encoded representation to the input for the next layer to start loop again
            x_train_tmp = encoded_train 
            x_test_tmp = encoded_test
             
        # Fine-turning
        
        model = Sequential()
        # Get a sequense of all pre-trained encoders into a deep structure
        for encoder in encoders:
            model.add(encoder)
        # Add supervised model by attaching 20-NN where input is a product of training Deep structure
        model.add(Dense(nb_hidden_layers[-1], activation='sigmoid'))
        model.add(Dense(20, activation="sigmoid"))
        model.add(Dense(1, activation="linear"))
        model.compile(loss='mse', optimizer="sgd")
    
        model.summary()
    
        # Train model
        history=model.fit(x_train_cvcv, y_train_cv, batch_size=64, nb_epoch=200, validation_data=(x_test, y_test))
    
        # Visualize training process
        plt.plot(history.history['loss'])
        plt.plot(history.history['val_loss'])
        plt.ylabel('loss')
        plt.xlabel('epoch')
        plt.legend(['train', 'test'], loc='upper left')
        plt.show()

        # Get predictions on the test-set by the entire structure
        test_predictions=model.predict(x_valid_cv)
    
        # Calculate metrics for each fold and append them to 10-dim array (10 folds)
        MSE_valid_folds.append(mean_squared_error(y_valid_cv, valid_predictions))
        R2_valid_folds.append(r2_score(y_valid_cv, valid_predictions))
        MAE_valid_folds.append(mean_absolute_error(y_valid_cv,valid_predictions))
            
    print("MSE_valid =", MSE_valid_folds)
    print("R2_valid=", R2_valid_folds) 
    print("MAE_valid=", MAE_valid_folds)
    
    # Calculate median-average of metrics from 10 folds and append to 5-dim array (5 trials)
    MSE_test_5.append(median(MSE_valid_folds))
    R2_test_5.append(median(R2_valid_folds))
    MAE_test_5.append(median(MAE_valid_folds))

print("MSE_test_5 =", MSE_test_5)
print("R2_test_5 =", R2_test_5) 
print("MAE_test_5 =", MAE_test_5) 
   
# Calculate median-average of metrics from 5 trials 
MSE_final=median(MSE_test_5)
print(MSE_final)
R2_final=median(R2_test_5)
print(R2_final)
MAE_final=median(MAE_test_5)
print(MAE_final)

# Calculate total time of algorithm running
end_time = timeit.default_timer()
hours, rem = divmod(end_time-start_time, 3600)
minutes, seconds = divmod(rem, 60)
print ("{:0>2}:{:0>2}:{:05.2f}".format(int(hours),int(minutes),seconds))
    
